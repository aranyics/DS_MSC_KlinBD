{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning methods\n",
    "\n",
    "This notebook demonstrates basic methods for machine learning on tabular data. Need python modules scikit-learn, sweetviz, lightgbm, optuna and tpot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pandas api\n",
    "from pandas.api.types import is_string_dtype, is_object_dtype\n",
    "\n",
    "#encoders and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "#feature engineering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#feature importance, feature selection\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#exploratory data analysis\n",
    "import sweetviz\n",
    "from sweetviz.feature_config import FeatureConfig\n",
    "\n",
    "#models\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor #sudo apt install libgomp1\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "#model interpretation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, PredictionErrorDisplay\n",
    "\n",
    "#model optimization\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "#autoML\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred, output_dict=True):\n",
    "    \"\"\"Generates a regression report similar to classification_report in sklearn.metrics.\n",
    "    The explained variance, r2 score, mean absolute error (MAE), mean squared error (MSE) and square root mean squared error (RMSE) are reported.\n",
    "\n",
    "    :param y_true: Ground truth (correct) target values.\n",
    "    :type y_true: array, shape = [n_samples]\n",
    "    :param y_pred: Estimated targets as returned by model.\n",
    "    :type y_pred: array, shape = [n_samples]\n",
    "    :param output_dict: Return a dictionary of reported scores, defaults to True.\n",
    "    :type output_dict: bool, optional\n",
    "    :return: Regression report\n",
    "    :rtype: dict\n",
    "    \"\"\"    \n",
    "\n",
    "    import sklearn.metrics as metrics\n",
    "    import numpy as np\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    #mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    #median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    if output_dict is True:\n",
    "        report = {\n",
    "            'explained_variance' : round(explained_variance,4),\n",
    "            #'mean_squared_log_error' : round(mean_squared_log_error,4),\n",
    "            'R2' : round(r2,4),\n",
    "            'MAE' : round(mean_absolute_error,4),\n",
    "            'MSE' : round(mse,4),\n",
    "            'RMSE': round(np.sqrt(mse),4)\n",
    "        }\n",
    "        return report\n",
    "    else:\n",
    "        print('explained_variance: ', round(explained_variance,4))    \n",
    "        #print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "        print('R2: ', round(r2,4))\n",
    "        print('MAE: ', round(mean_absolute_error,4))\n",
    "        print('MSE: ', round(mse,4))\n",
    "        print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv\n",
    "(or load through databricks-connect SparkSQL session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stat_med_acs.csv').sort_values(by='value').reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct column names, eliminate special characters and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames = {c: re.sub(r\"[^a-zA-Z0-9]+\", ' ', c).replace(' ', '_') for c in df.columns.tolist()}\n",
    "\n",
    "df = df.rename(columns=new_colnames)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Label Encoder\n",
    "\n",
    "Used for encode target labels for classification to monotonically increasing numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "enc.fit(df['stage'])\n",
    "\n",
    "enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !! Note that the order of classes don't reflect severity of kidney failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import column_or_1d\n",
    "\n",
    "# Custom encoder class\n",
    "class MyLabelEncoder(LabelEncoder):\n",
    "\n",
    "    # Override fit method\n",
    "    def fit(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        self.classes_ = pd.Series(y).unique()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenc = MyLabelEncoder()\n",
    "\n",
    "myenc.fit(df['stage'])\n",
    "\n",
    "myenc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stage'] = myenc.transform(df['stage'])\n",
    "df.iloc[:,:8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ordinal Encoder\n",
    "\n",
    "Similar to Label Encoder with handling of unknown and missing values in transformed data\n",
    "\n",
    "Note: expects 2-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "\n",
    "df['sex_encoded'] = enc.fit_transform(df['sex'].to_numpy().reshape(-1, 1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "transformed = enc.fit_transform(df['sex'].to_numpy().reshape(-1, 1))\n",
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = enc.get_feature_names_out()\n",
    "for i in range(len(feature_names)):\n",
    "    if feature_names[i].endswith('nan') or feature_names[i].endswith('None') or feature_names[i].endswith('_U'):\n",
    "        continue\n",
    "    f = '_'.join(['sex'] + feature_names[i].split('_')[1:])\n",
    "    df[f] = transformed.todense()[:,i]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['patient_id', 'sex_encoded', 'sex_F', 'sex_M'], axis=1, errors='ignore')\n",
    "\n",
    "enc = LabelEncoder()\n",
    "df['sex'] = enc.fit_transform(df['sex'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data for faster computations\n",
    "_, df_reduced = train_test_split(df, test_size=0.2*0.2, stratify=df['stage'], random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test = train_test_split(df_reduced, test_size=0.2, stratify=df_reduced['stage'], random_state=42)\n",
    "\n",
    "# Make regression data\n",
    "Xr_train = X_train.copy().drop(columns=['stage'], axis=1)\n",
    "Xr_test  = X_test.copy().drop(columns=['stage'], axis=1)\n",
    "Xr_train.to_csv('stat_med_acs_regr_train.csv', index=False)\n",
    "Xr_train.to_csv('stat_med_acs_regr_test.csv', index=False)\n",
    "\n",
    "yr_train = Xr_train['value']\n",
    "Xr_train = Xr_train.drop(columns=['value'], axis=1)\n",
    "yr_test = Xr_test['value']\n",
    "Xr_test = Xr_test.drop(columns=['value'], axis=1)\n",
    "\n",
    "# Make classification data\n",
    "Xc_train = X_train.copy().drop(columns=['value'], axis=1)\n",
    "Xc_test  = X_test.copy().drop(columns=['value'], axis=1)\n",
    "Xc_train.to_csv('stat_med_acs_clas_train.csv', index=False)\n",
    "Xc_train.to_csv('stat_med_acs_clas_test.csv', index=False)\n",
    "\n",
    "yc_train = Xc_train['stage']\n",
    "Xc_train = Xc_train.drop(columns=['stage'], axis=1)\n",
    "yc_test = Xc_test['stage']\n",
    "Xc_test = Xc_test.drop(columns=['stage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "**Feature importance** provides predicted measures to compare the influence of features on a specific modeling task. Some of the more popular techniques:\n",
    "1. **Permutated feature importance**, which uses an arbitrary regression or classification modeling algorithm, and measures its performance every time we permutate a single feature to compare performance between models with the original feature and the permutated one. Permutating every feature multiple times allows to predict their influence in the current task accuartely.\n",
    "2. **SHAP values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFI(features, importance, target='', modelname='', scorer='', n=20):\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    if n is not None and isinstance(n, int):\n",
    "        features = features[:n]\n",
    "        importance = importance[:n]\n",
    "    \n",
    "    importance = [round(fi, ndigits=2) for fi in importance]\n",
    "\n",
    "    fig, ax = pyplot.subplots(layout='constrained')\n",
    "    bars = ax.bar([x for x in range(len(importance))], importance)\n",
    "    ax.bar_label(bars, padding=3, rotation=75)\n",
    "    ax.set_title('Feature importance (target: {0})\\n{1} - {2}'.format(target, modelname, scorer))\n",
    "    ax.set_xticks(np.arange(len(features)), features)\n",
    "    ax.set_ylim(0, np.max(importance)*1.2)\n",
    "    pyplot.xticks(rotation=90)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model to fit adta with\n",
    "model = LGBMRegressor()\n",
    "scorer = 'neg_root_mean_squared_error'\n",
    "\n",
    "# Fit model\n",
    "model.fit(Xr_train, yr_train)\n",
    "\n",
    "# Fit model with permutated data to assess permutation-based feature importance\n",
    "FI = permutation_importance(model, Xr_train, yr_train, scoring=scorer, n_repeats=5)\n",
    "\n",
    "# Sort importances\n",
    "importance = FI.importances_mean\n",
    "order = sorted(range(len(importance)), key=lambda k: importance[k], reverse=True)\n",
    "\n",
    "importance_order = [importance[i] for i in order]\n",
    "features_order = [Xr_train.columns[i] for i in order]\n",
    "\n",
    "# Plot feature importance\n",
    "plotFI( features_order, importance_order, 'value', model.__class__.__name__, scorer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model to fit adta with\n",
    "model = LGBMClassifier()\n",
    "scorer = 'neg_log_loss'\n",
    "\n",
    "# Fit model\n",
    "model.fit(Xc_train, yc_train)\n",
    "\n",
    "# Fit model with permutated data to assess permutation-based feature importance\n",
    "FI = permutation_importance(model, Xc_train, yc_train, scoring=scorer, n_repeats=5)\n",
    "\n",
    "# Sort importances\n",
    "importance = FI.importances_mean\n",
    "order = sorted(range(len(importance)), key=lambda k: importance[k], reverse=True)\n",
    "\n",
    "importance_order = [importance[i] for i in order]\n",
    "features_order = [Xc_train.columns[i] for i in order]\n",
    "\n",
    "# Plot feature importance\n",
    "plotFI( features_order, importance_order, 'stage', model.__class__.__name__, scorer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction (<a href=\"https://en.wikipedia.org/wiki/Feature_selection\">Wikipedia</a>).\n",
    "\n",
    "Collection of various feature selection methods: https://github.com/mlpapers/feature-selection\n",
    "\n",
    "In this example we select the top 20 features in feature importance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc_train = Xc_train[features_order[0:20]]\n",
    "Xc_test = Xc_test[features_order[0:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Explorative data analysis provides data visualization for the following purposes:\n",
    "- View distribution of variables\n",
    "- Review data quality (missing values, data type)\n",
    "- Understand associations between variables used as features\n",
    "\n",
    "We use SweetViz for EDA (https://github.com/fbdesignpro/sweetviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc_train['stage'] = yc_train\n",
    "Xc_test['stage'] = yc_test\n",
    "\n",
    "# Force target variable to be numeric\n",
    "sweetviz_config = FeatureConfig( force_num=['stage'] )\n",
    "\n",
    "# One set EDA\n",
    "#sweetviz_report = sweetviz.analyze([Xc_train, 'medication train.']\n",
    "#                                   target_feat='stage', feat_cfg=sweetviz_config, pairwise_analysis='on')\n",
    "\n",
    "# Two set comparison\n",
    "sweetviz_report = sweetviz.compare([Xc_train, 'medication train.'], [Xc_test, 'mediacation test'],\n",
    "                                   target_feat='stage', feat_cfg=sweetviz_config, pairwise_analysis='on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweetviz_report.show_notebook(w='100%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc_train = Xc_train.drop(columns=['stage'], axis=1)\n",
    "Xc_test = Xc_test.drop(columns=['stage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model_lrg = LogisticRegression()\n",
    "\n",
    "# Fit model\n",
    "model_lrg.fit(Xc_train, yc_train)\n",
    "\n",
    "# Predict 'stage' on test set\n",
    "yc_pred = model_lrg.predict(Xc_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report = classification_report(yc_test, yc_pred, output_dict=True)\n",
    "print('Accuracy: ' + str(report['accuracy']))\n",
    "print(report['weighted avg'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    model_lrg, Xc_test, yc_test, display_labels=myenc.classes_.tolist(), cmap=plt.cm.Blues, normalize=None\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model_rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "model_rfc.fit(Xc_train, yc_train)\n",
    "\n",
    "# Predict 'stage' on test set\n",
    "yc_pred = model_rfc.predict(Xc_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report = classification_report(yc_test, yc_pred, output_dict=True)\n",
    "print('Accuracy: ' + str(report['accuracy']))\n",
    "print(report['weighted avg'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    model_rfc, Xc_test, yc_test, display_labels=myenc.classes_.tolist(), cmap=plt.cm.Blues, normalize=None\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of fitted model\n",
    "plotFI( model_rfc.feature_names_in_, model_rfc.feature_importances_, target='stage', modelname=model_rfc.__class__.__name__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Boosting Classifier (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model_lgb = LGBMClassifier()\n",
    "\n",
    "# Fit model\n",
    "model_lgb.fit(Xc_train, yc_train)\n",
    "\n",
    "# Predict 'stage' on test set\n",
    "yc_pred = model_lgb.predict(Xc_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report = classification_report(yc_test, yc_pred, output_dict=True)\n",
    "print('Accuracy: ' + str(report['accuracy']))\n",
    "print(report['weighted avg'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    model_lgb, Xc_test, yc_test, display_labels=myenc.classes_.tolist(), cmap=plt.cm.Blues, normalize=None\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of fitted model\n",
    "plotFI( model_lgb.feature_name_, model_lgb.feature_importances_, target='stage', modelname=model_lgb.__class__.__name__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model_lgbr = LGBMRegressor()\n",
    "\n",
    "# Fit model\n",
    "model_lgbr.fit(Xr_train, yr_train)\n",
    "\n",
    "# Predict 'stage' on test set\n",
    "yr_pred = model_lgbr.predict(Xr_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report_regr = regression_report(yr_test, yr_pred, output_dict=True)\n",
    "print(report_regr)\n",
    "\n",
    "# Plot regression error\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    yr_test,\n",
    "    y_pred=yr_pred,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    subsample=100,\n",
    "    ax=axs[0],\n",
    "    random_state=0,\n",
    ")\n",
    "axs[0].set_title(\"Actual vs. Predicted values\")\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    yr_test,\n",
    "    y_pred=yr_pred,\n",
    "    kind=\"residual_vs_predicted\",\n",
    "    subsample=100,\n",
    "    ax=axs[1],\n",
    "    random_state=0,\n",
    ")\n",
    "axs[1].set_title(\"Residuals vs. Predicted Values\")\n",
    "fig.suptitle(\"Plotting regression predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of fitted model\n",
    "plotFI( model_lgbr.feature_name_, model_lgbr.feature_importances_, target='stage', modelname=model_lgbr.__class__.__name__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **Optuna** hyperparameter optimization framework, we can optimize the model parameters for better fit. This needs definition of objective function to minimize (or maximize, based on the scoring method), with suggestions on parameter ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lrc(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1'])\n",
    "    if penalty == 'l1':\n",
    "        solver = 'saga'\n",
    "    else:\n",
    "        solver = 'lbfgs'\n",
    "    regularization = trial.suggest_uniform('logistic-regularization', 0.01, 10)\n",
    "\n",
    "    # Create and fit model\n",
    "    model = LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        C=regularization,\n",
    "        solver=solver,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(Xc_train, yc_train)\n",
    "\n",
    "    # Make predictions and calculate score\n",
    "    yc_pred = model.predict(Xc_test)\n",
    "    acc = accuracy_score(yc_test, yc_pred)\n",
    "\n",
    "    # Return Accuracy\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rfc(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Create and fit model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(Xc_train, yc_train)\n",
    "\n",
    "    # Make predictions and calculate score\n",
    "    yc_pred = model.predict(Xc_test)\n",
    "    acc = accuracy_score(yc_test, yc_pred)\n",
    "\n",
    "    # Return Accuracy\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimization study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study object\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "\n",
    "# Run optimization process\n",
    "study.optimize(objective_rfc, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter importance\n",
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slice plot\n",
    "vis.plot_slice(study, params=[\"n_estimators\", \"max_depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contour plot\n",
    "vis.plot_contour(study, params=[\"min_samples_split\", \"min_samples_leaf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parallel_coordinate\n",
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best trial and best hyperparameters\n",
    "print(\"Best trial:\", study.best_trial)\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model_rfc = RandomForestClassifier(**study.best_params)\n",
    "\n",
    "# Fit model\n",
    "model_rfc.fit(Xc_train, yc_train)\n",
    "\n",
    "# Predict 'stage' on test set\n",
    "yc_pred = model_rfc.predict(Xc_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report = classification_report(yc_test, yc_pred, output_dict=True)\n",
    "print('Accuracy: ' + str(report['accuracy']))\n",
    "print(report['weighted avg'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    model_rfc, Xc_test, yc_test, display_labels=myenc.classes_.tolist(), cmap=plt.cm.Blues, normalize=None\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance of fitted model\n",
    "plotFI( model_rfc.feature_names_in_, model_rfc.feature_importances_, target='stage', modelname=model_rfc.__class__.__name__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Boosting Classifier (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbc(trial):\n",
    "    param = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_class\": 3,\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(**param)\n",
    "\n",
    "    model.fit(Xc_train, yc_train)\n",
    "\n",
    "    # Make predictions and calculate score\n",
    "    yc_pred = model.predict(Xc_test)\n",
    "    acc = accuracy_score(yc_test, yc_pred)\n",
    "\n",
    "    # Return Accuracy\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regression objective function examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rfr(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Create and fit model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(Xr_train, yr_train)\n",
    "\n",
    "    # Make predictions and calculate RMSE\n",
    "    yr_pred = model.predict(Xr_test)\n",
    "    rmse = np.sqrt(mean_squared_error(yr_test, yr_pred))\n",
    "    mae = mean_absolute_error(yr_test, yr_pred)\n",
    "    r2 = r2_score(yr_test, yr_pred)\n",
    "\n",
    "    # Return RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbr(trial):\n",
    "    param = {\n",
    "        'metric': 'rmse', \n",
    "        'random_state': 48,\n",
    "        'n_estimators': 20000,\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n",
    "        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(**param)\n",
    "\n",
    "    model.fit(Xr_train, yr_train)\n",
    "\n",
    "    # Make predictions and calculate RMSE\n",
    "    yr_pred = model.predict(Xr_test)\n",
    "    rmse = np.sqrt(mean_squared_error(yr_test, yr_pred))\n",
    "    mae = mean_absolute_error(yr_test, yr_pred)\n",
    "    r2 = r2_score(yr_test, yr_pred)\n",
    "\n",
    "    # Return RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Machine Learning\n",
    "\n",
    "with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TPOT classifier\n",
    "model = TPOTClassifier(\n",
    "    max_time_mins=15,\n",
    "    cv=5,\n",
    "    scoring='neg_log_loss',\n",
    "    random_state=42,\n",
    "    verbosity=2\n",
    "    )\n",
    "\n",
    "# Optimize and fit pipelines\n",
    "model.fit(Xc_train, yc_train)\n",
    "\n",
    "# Export best pipeline and fitted model\n",
    "model.export('stat_med_acs_clas_pipeline.txt')\n",
    "pickle.dump( model.fitted_pipeline_, open( 'stat_med_acs_clas_model.pkl', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 'stage' on test set\n",
    "yc_pred = model.predict(Xc_test)\n",
    "\n",
    "# Report classification metrics\n",
    "report = classification_report(yc_test, yc_pred, output_dict=True)\n",
    "print('Accuracy: ' + str(report['accuracy']))\n",
    "print(report['weighted avg'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    model.fitted_pipeline_, Xc_test, yc_test, display_labels=myenc.classes_.tolist(), cmap=plt.cm.Blues, normalize=None\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
