{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Connect\n",
    "\n",
    "This guide shows how to **manage Databricks workspace** using the Python API, and **access your database** on the SQL Warehouse by connecting to a Compute cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Databricks\n",
    "\n",
    "This notebook uses data stored on a Databricks Data Warehouse. To access these data, an access to Databricks and the specific data catalogue is required. This link provide useful information to access Databricks remotely:\n",
    "- https://docs.databricks.com/en/dev-tools/databricks-connect/python/index.html\n",
    "\n",
    "Given these requirements are fulfilled, do the following steps:\n",
    "\n",
    "#### 1. Learn Databricks host\n",
    "- The **host** is the browser URL of Databricks web interface. Example:\n",
    "    - https://[hostID].azuredatabricks.net/\n",
    "\n",
    "**IMPORTANT:** The full **host** (the URL) is needed later, not only the hostID.\n",
    "\n",
    "#### 2. Gain access token in Databricks web\n",
    "- User Settings -> Developer -> Access tokens (Manage) -> Generate new token\n",
    "- Specify Comment and Lifetime (leave empty for no expiration date)\n",
    "- Save the generated **token** (you will not be able to see it again)\n",
    "\n",
    "#### 3. Acquire SQL Warehouse ID\n",
    "- Look for a warehouse in the SQL Warehouses menu\n",
    "- Save the ID in the warehouse Overview: Name (ID: **warehouse_ID**)\n",
    "\n",
    "#### 4. Set up compute cluster in Databricks web, that have access to the warehouse\n",
    "- In Compute, create, or look for a compute node with this Runtime environment:\n",
    "    - 13.0 +\n",
    "    - 13.0 ML +\n",
    "- Select node -> More ... -> View JSON\n",
    "- Save the value of **\"cluster_id\"**\n",
    "\n",
    "#### 5. Install databricks-connect in your Python environment\n",
    "**Install** the databricks-connect package specific to the Compute cluster used in Databricks. Example:\n",
    "- pip install databricks-connect==13.2\n",
    "\n",
    "Note: databricks-connect 13.x+ requires python 3.10+, thus it is recommended to build the analysis environment with python 3.10+\n",
    "\n",
    "To **upgrade** this package, use the *--upgrade* option:\n",
    "- pip install --upgrade \"databricks-connect==14.3.*\"\n",
    "\n",
    "Note: with the *.\\** you can ensure that the latest revision of the package version will be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Databricks Connection\n",
    "\n",
    "To configure, we need these information:\n",
    "- Databricks host\n",
    "- Access token\n",
    "- Compute cluster ID\n",
    "\n",
    "Optional:\n",
    "- Warehouse ID can be included in the Config object for easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.core import Config\n",
    "\n",
    "config = Config(\n",
    "    profile    = 'access', # arbitrary config profile name\n",
    "    host       = '----',   # fill this in\n",
    "    token      = '----',   # fill this in\n",
    "    cluster_id = '----'    # fill this in\n",
    "    warehouse_id = '----'    # fill this in\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manage, and Start SQL Warehouse and Compute cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a WorkspaceClient\n",
    "\n",
    "For more information, refer to the Python API on the WorkspaceClient class:\n",
    "- https://databricks-sdk-py.readthedocs.io/en/latest/clients/workspace.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (optional) Get general workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print clusters\n",
    "print([[i.cluster_name, i.cluster_id] for i in w.clusters.list()])\n",
    "\n",
    "# Print warehouses\n",
    "print([[i.name, i.id] for i in w.warehouses.list()])\n",
    "\n",
    "# Print catalogs (1st level)\n",
    "print([i.full_name for i in w.catalogs.list()])\n",
    "\n",
    "# Print schemas in catalogs (2nd level)\n",
    "print([i.full_name for i in w.schemas.list(catalog_name='medications')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster information (Fill in cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.clusters.get(cluster_id='----').as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warehouse information (Fill in id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.warehouses.get(id='----').as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalog and schema information (Fill in name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.catalogs.get(name='----').as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.schemas.get(full_name='----.----').as_dict() # full_name in the format: catalog.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table information (Fill in names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may not work (API bug?)\n",
    "w.tables.list(catalog_name='----', schema_name='----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_name = '----'\n",
    "schema_name = '----'\n",
    "table_name = '----'\n",
    "\n",
    "# Print table comments\n",
    "print(w.tables.get(full_name='%s.%s.%s' % (catalog_name, schema_name, table_name)).comment)\n",
    "\n",
    "# Print table columns and data types\n",
    "[[c['name'], c['type_name']] for c in w.tables.get(full_name='%s.%s.%s' % (catalog_name, schema_name, table_name)).as_dict()['columns']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we begin starting the Compute node, then move on.\n",
    "w.clusters.start(cluster_id=config.cluster_id)\n",
    "\n",
    "# Second, we start the SQL Warehouse, and wait until it's finished. Fill in warehouse id.\n",
    "w.warehouses.start_and_wait(id='----')\n",
    "\n",
    "# Lastly, we wait to get 'RUNNING' state from the Compute node, too.\n",
    "w.clusters.wait_get_cluster_running(cluster_id=config.cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (optional) Check whether the clusters are running.\n",
    "\n",
    "For this, we define two functions that check cluster states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDatabricksStopped(w, warehouse_id=None, cluster_id=None):\n",
    "    if warehouse_id is not None:\n",
    "        try:\n",
    "            print('Warehouse: ' + w.warehouses.wait_get_warehouse_stopped(id=warehouse_id).as_dict()['state'])\n",
    "        except:\n",
    "            print('Warehouse might be running')\n",
    "    if cluster_id is not None:\n",
    "        try:\n",
    "            print('Cluster: ' + w.clusters.wait_get_cluster_terminated(cluster_id=cluster_id).as_dict()['state'])\n",
    "        except:\n",
    "            print('Cluster might be running')\n",
    "\n",
    "def checkDatabricksRunning(w, warehouse_id=None, cluster_id=None):\n",
    "    if warehouse_id is not None:\n",
    "        try:\n",
    "            print('Warehouse: ' + w.warehouses.wait_get_warehouse_running(id=warehouse_id).as_dict()['state'])\n",
    "        except:\n",
    "            print('Warehouse might be stopped')\n",
    "    if cluster_id is not None:\n",
    "        try:\n",
    "            print('Cluster: ' + w.clusters.wait_get_cluster_running(cluster_id=cluster_id).as_dict()['state'])\n",
    "        except:\n",
    "            print('Cluster might be terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDatabricksRunning(w, warehouse_id='----', cluster_id=config.cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query and export data from Databricks\n",
    "\n",
    "**IMPORTANT**: To connect via Spark session, the Compute cluster **AND** the SQL Warehouse must be running in Databricks. **Please, stop these computers after data is accessed**, if automated termination is not set up on these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make spark session on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show description of data tables\n",
    "\n",
    "With the DESCRIBE statement in SQL some information of each column can be extracted: the column name, data type and column comments. This is usually faster than querying the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DESCRIBE TABLE catalog.database.table \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Query from SQL Warehouse and convert to Pandas DataFrame\n",
    "\n",
    "Note: Both the Spark and Pandas DataFrames can be used to create an AMLDataVariant object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_spark = spark.sql(\"\"\" SELECT * FROM catalog.database.table \"\"\")\n",
    "\n",
    "df = df_in_spark.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
