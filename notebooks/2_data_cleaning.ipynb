{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.core import Config\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "config = Config(\n",
    "    profile    = 'access', # arbitrary config profile name\n",
    "    host       = '----',   # fill this in\n",
    "    token      = '----',   # fill this in\n",
    "    cluster_id = '----'    # fill this in\n",
    "    warehouse_id = '----'    # fill this in\n",
    ")\n",
    "\n",
    "w = WorkspaceClient(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.clusters.start(cluster_id=config.cluster_id)\n",
    "w.warehouses.start_and_wait(id=config.warehouse_id).as_dict()['state']\n",
    "w.clusters.wait_get_cluster_running(cluster_id=config.cluster_id).as_dict()['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstNumber(x: str):\n",
    "    import re\n",
    "    #tmp = ['29,84 mL/p/1.73m2', 'lásd megj.', '>-90 mL/p/1.73m2']\n",
    "    try:\n",
    "        return float(re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", x.split(' ')[0].replace(',', '.').strip('.'))[0])\n",
    "    except:\n",
    "        return float(-1111.0001)\n",
    "    \n",
    "def getNormRange(x: str):\n",
    "    import re\n",
    "    #tmp = ['29,84 mL/p/1.73m2', 'lásd megj.', '>-90 mL/p/1.73m2']\n",
    "    if x is None:\n",
    "        return None, None\n",
    "    x = x.strip(' ')\n",
    "    if x.startswith('- '): x = x.replace('- ', '<')\n",
    "    if x.endswith('-') or x.endswith(' -'): x = '>'+x\n",
    "    parts = x.replace(', ', ' ').replace(' - ', ' ').replace('- ', ' ').replace(' -', ' ').split(' ')\n",
    "    modif = parts.copy()\n",
    "    for i in range(len(parts)):\n",
    "        parts[i] = getFirstNumber(parts[i])\n",
    "    modif = [re.sub(r'[0-9]', '', modif[k].replace('.','').replace('-','').replace(',','')) for k in range(len(parts)) if parts[k] != -1111.0001]\n",
    "    parts = [parts[k] for k in range(len(parts)) if parts[k] != -1111.0001]\n",
    "    if len(parts) == 2:\n",
    "        if modif[0] not in ['<', '>']: modif[0] = '>'\n",
    "        if modif[1] not in ['<', '>']: modif[1] = '<'\n",
    "    return modif, parts\n",
    "\n",
    "def getNormUpper(x: str):\n",
    "    r_mod, r_num = getNormRange(x)\n",
    "    try:\n",
    "        return max([r_num[i] for i in range(len(r_num)) if r_mod[i]=='<'])\n",
    "    except:\n",
    "        import numpy as np\n",
    "        return np.nan\n",
    "\n",
    "def getNormLower(x: str):\n",
    "    r_mod, r_num = getNormRange(x)\n",
    "    #if len(r_num) == 2 and min(r_num) == max(r_num):\n",
    "    #    return None\n",
    "    try:\n",
    "        return min([r_num[i] for i in range(len(r_num)) if r_mod[i]=='>'])\n",
    "    except:\n",
    "        import numpy as np\n",
    "        return np.nan\n",
    "\n",
    "rawToValue = F.udf( lambda x : getFirstNumber(x), FloatType() )\n",
    "normUpper = F.udf( lambda x : getNormUpper(x), FloatType() )\n",
    "normLower = F.udf( lambda x : getNormLower(x), FloatType() )\n",
    "removeNumbersFromString = F.udf( lambda x : re.sub(r'[0-9]', '', x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "\n",
    "class extendSparkDataFrameMixin(object):\n",
    "    @classmethod\n",
    "    def ext(cls, obj):\n",
    "        obj.__class__ = cls\n",
    "        return(obj)\n",
    "\n",
    "class DataFrameExt(DataFrame, extendSparkDataFrameMixin):\n",
    "    \n",
    "    def check(self, n=20, **kwargs):\n",
    "        print(self.count())\n",
    "        self.show(n=n, **kwargs)\n",
    "\n",
    "    def duplicateRecords(self, *cols):\n",
    "        if len(cols) == 1:\n",
    "            cols = cols[0]\n",
    "        not_duplicate_records = self.groupBy(cols).count().where('count = 1').drop('count')\n",
    "        duplicate_records = self.join(not_duplicate_records, on=cols, how='left_anti')\n",
    "        return type(self).ext(duplicate_records) #DataFrameExt(duplicate_records, self._session)\n",
    "\n",
    "    def toDate(self, col):\n",
    "        return type(self).ext( self.withColumn(col, F.when( F.col(col).rlike('\\d{4}-\\d{1,2}-\\d{1,2}'), F.to_date(col, 'yyyy-M-d') )) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab = spark.sql(\"\"\" SELECT * FROM ds_msc_2024_spring.medication.labor WHERE requested_gfr = 1 \"\"\")\n",
    "df_med = spark.sql(\"\"\" SELECT * FROM ds_msc_2024_spring.medication.medication \"\"\")\n",
    "df_pat = spark.sql(\"\"\" SELECT * FROM ds_msc_2024_spring.medication.patients \"\"\")\n",
    "df_pre = spark.sql(\"\"\" SELECT * FROM ds_msc_2024_spring.medication.prescription \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DESCRIBE TABLE ds_msc_2024_spring.medication.patients \"\"\").show()\n",
    "df_pat.show(n=5)\n",
    "df_pat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplicates and other anomalies in *'patient_id'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_records = DataFrameExt.ext( df_pat ).duplicateRecords(['patient_id']).orderBy('patient_id')\n",
    "duplicate_records.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates with anti-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat_cleaned = df_pat.join(duplicate_records, on='patient_id', how='left_anti').orderBy('patient_id')\n",
    "df_pat_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medication table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DESCRIBE TABLE ds_msc_2024_spring.medication.medication \"\"\").show()\n",
    "df_med.show(n=5)\n",
    "df_med.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_records = DataFrameExt.ext( df_med ).duplicateRecords('standardized_name')\n",
    "duplicate_records.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescription table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DESCRIBE TABLE ds_msc_2024_spring.medication.prescription \"\"\").show()\n",
    "df_pre.show(n=5)\n",
    "df_pre.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove records with *'patient_id'* not in the **Patients table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_cleaned = df_pre.join(df_pat_cleaned, on='patient_id', how='inner').drop('year_of_birth', 'sex')\n",
    "#df_pre_cleaned.show(5)\n",
    "df_pre_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplicates, and keep only the first prescription date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK = ['patient_id', 'standardized_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_records = DataFrameExt.ext( df_pre_cleaned ).duplicateRecords(PK)\n",
    "duplicate_records.show()\n",
    "duplicate_records.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK = ['patient_id', 'standardized_name', 'from_date']\n",
    "df_pre_cleaned = df_pre_cleaned.orderBy(PK).dropDuplicates(['patient_id', 'standardized_name'])\n",
    "df_pre_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labor table - GFR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DESCRIBE TABLE ds_msc_2024_spring.medication.labor \"\"\").show()\n",
    "df_lab.show(n=5)\n",
    "df_lab.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Know your data\"\n",
    "\n",
    "Show some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab.groupBy('department').count().show()\n",
    "df_lab.groupBy('mrkeyword').count().show()\n",
    "df_lab.groupBy('description').count().show()\n",
    "df_lab.groupBy('normal_range').count().show()\n",
    "df_lab.groupBy('unit').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data that are not explained trivially\n",
    "\n",
    "Duplicates for different set of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK_examdate = ['patient_id', 'examination_date']\n",
    "PK_dept = ['patient_id', 'examination_date', 'department']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_duplicate_examdate = df_lab.groupBy(PK_examdate).count().where('count = 1').drop('count')\n",
    "duplicate_examdate = df_lab.join(not_duplicate_examdate, on=PK_examdate, how='left_anti').orderBy(PK_examdate)\n",
    "duplicate_examdate.show(n=5)\n",
    "print(duplicate_examdate.count())\n",
    "\n",
    "not_duplicate_dept = df_lab.groupBy(PK_dept).count().where('count = 1').drop('count')\n",
    "duplicate_dept = df_lab.join(not_duplicate_dept, on=PK_dept, how='left_anti').orderBy(PK_examdate)\n",
    "duplicate_dept.show(n=5)\n",
    "print(duplicate_dept.count())\n",
    "\n",
    "duplicate_diff_dept = duplicate_examdate.join(duplicate_dept, on=PK_dept, how='left_anti').orderBy(PK_dept)\n",
    "duplicate_diff_dept.show(n=5)\n",
    "duplicate_diff_dept.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab.filter(df_lab['normal_range'].isNull()).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab.filter(df_lab['unit'].isNull()).show()\n",
    "df_lab.filter(df_lab['unit'].isNull()).join(df_lab.filter(df_lab['rawvalue'] == \"Nem ért.\"), on=PK_dept, how='left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop records\n",
    "\n",
    "- Where *'unit'* is null, no observation is available\n",
    "- Drop records with *'patient_id'* not in **Patients table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab.na.drop(subset=['unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.join(df_pat_cleaned, on='patient_id', how='inner').orderBy(PK_examdate).drop('year_of_birth', 'sex')\n",
    "df_lab_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform unclean data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract observation from *'rawvalue'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.withColumn('value', rawToValue(F.col('rawvalue'))).filter(F.col('value') != -1111.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct normal range and convert to numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.withColumn('norm_minimum', normLower(F.col('normal_range'))) \\\n",
    "                               .withColumn('norm_maximum', normUpper(F.col('normal_range')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned.filter(df_lab_cleaned['normal_range'].isNull()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add categories of numerical data\n",
    "\n",
    "Stage of kidney disease from clinical experts:\n",
    "\n",
    "    Stádium Jellemző GFR ml/perc /1,73m2 Gyakoriság (%)\n",
    "\n",
    "        1. vesebetegség norm. v. magas GFR-rel >90 3,3\n",
    "\n",
    "        2. enyhe vesebetegség csökkent GFR-rel 60-89 3,0\n",
    "\n",
    "        3. mérsékelt veseelégtelenség 30-59 4,3\n",
    "\n",
    "        4. súlyos veseelégtelenség 15-29 0,2\n",
    "\n",
    "        5. végstádiumú veseelégtelenség <15 v. dialízis 0,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr_ranges = {'norm_high': [90.0, 100000.0],\n",
    "              'mild': [60.0, 90.0],\n",
    "              'moderate': [30.0, 60.0],\n",
    "              'severe': [15.0, 30.0],\n",
    "              'failure': [-1000000.0, 15.0]}\n",
    "\n",
    "gfrToStage = F.udf(lambda x : [k for k in gfr_ranges.keys() if x >= gfr_ranges[k][0] and x < gfr_ranges[k][1]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.withColumn('stage', gfrToStage(F.col('value')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop measurements for patients on the same date with the same stage of kidney malfunction\n",
    "\n",
    "Note: other conditions may arise (e.g. keep the highest GFR value from each examination), which need prior consultation with field experts, or the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK_stage = ['patient_id', 'examination_date', 'stage']\n",
    "df_lab_cleaned = df_lab_cleaned.dropDuplicates(subset=PK_stage)\n",
    "#df_lab_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.drop('description', 'observation', 'unit', 'normal_range', 'comment', 'rawvalue', 'requested_gfr')\n",
    "#df_lab_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *'examination_date'* column should be 'date' type instead of string. Look for any anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned.withColumn('datedelim', removeNumbersFromString(df_lab_cleaned[\"examination_date\"])).groupBy('datedelim').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned = df_lab_cleaned.withColumn('examination_date', F.when(F.col('examination_date').rlike('\\d{4}-\\d{1,2}-\\d{1,2}'), F.to_date('examination_date', 'yyyy-M-d')))\n",
    "df_lab_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_cleaned.select([F.count(F.when(\n",
    "    F.isnan(F.col(each_col)) | \\\n",
    "    (F.col(each_col) == \"\") | \\\n",
    "    F.isnull(F.col(each_col)) | \\\n",
    "    (F.lower(F.col(each_col)) == \"null\"), 1 \\\n",
    ")).alias(each_col) for each_col in ['patient_id', 'department', 'mrkeyword', 'value']]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic table - create new table for statistical analysis\n",
    "\n",
    "- Patient demography\n",
    "- GFR value (minumum, or most severe GFR)\n",
    "- Kidney disease stage\n",
    "- Medications prescribed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get GFR values and stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prescriptions for patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export table to csv for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clean database on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name='medication_acs'\n",
    "catalog_name='ds_msc_2024_spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.schemas.create(name=database_name, catalog_name=catalog_name, comment='First pass of data cleaning of ds_msc_2024_spring.medication database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register tables for SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat_cleaned.registerTempTable('c1_patients')\n",
    "df_med.registerTempTable('c1_medication')\n",
    "df_pre_cleaned.registerTempTable('c1_prescr')\n",
    "df_lab_cleaned.registerTempTable('c1_labor_gfr')\n",
    "df_stat.registerTempTable('c1_stat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload tables to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" CREATE TABLE ds_msc_2024_spring.medication_acs.patients AS SELECT * FROM c1_patients \"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE ds_msc_2024_spring.medication_acs.medication AS SELECT * FROM c1_medication \"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE ds_msc_2024_spring.medication_acs.prescription AS SELECT * FROM c1_prescr \"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE ds_msc_2024_spring.medication_acs.labor_gfr AS SELECT * FROM c1_labor_gfr \"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE ds_msc_2024_spring.medication_acs.stat AS SELECT * FROM c1_stat \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
